{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48624f2",
   "metadata": {},
   "source": [
    "Refer AmazonFoodReview in NLP folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9eac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import  pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2323e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('../../../NLP/AmazonFoodReviews/database.sqlite')\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\"\n",
    "SELECT * \n",
    "FROM Reviews \n",
    "WHERE Score != 3\n",
    "\"\"\", connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "    \n",
    "filtered_data['Score'] = filtered_data['Score'].map(parition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76438dc5",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "### Data Cleaning: Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f47213",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f575e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "sorted_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplication_data = sorted_data.drop_duplicates(subset={\n",
    "    'UserId',\n",
    "    'HelpfulnessNumerator',\n",
    "    'HelpfulnessDenominator',\n",
    "    'Score',\n",
    "    'Time',\n",
    "    'Summary',\n",
    "    'Text'\n",
    "},\n",
    "keep='first', inplace=False\n",
    ")\n",
    "\n",
    "deduplication_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa427d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplication_data['Id'].size/filtered_data['Id'].size *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with HelpfulnessNumerator greater than HelpfulnessDenominator\n",
    "final_data = deduplication_data[deduplication_data.HelpfulnessNumerator <= deduplication_data.HelpfulnessDenominator]\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626068c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cda7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "\n",
    "for idx, doc in enumerate(final_data['Text'].values):\n",
    "    review = re.sub(r'<.*?>', ' ', doc)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in stop_words]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "    if final_data['Score'].values[idx] == 'positive':\n",
    "        all_positive_words.append(review)\n",
    "    else:\n",
    "        all_negative_words.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95651727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da30b09d",
   "metadata": {},
   "source": [
    "# RNN Implementation with Two Vectorization Approaches\n",
    "\n",
    "We'll implement Simple RNN with two different vectorization methods:\n",
    "1. **Word2Vec**: Pre-trained word embeddings fed into RNN\n",
    "2. **Embedding Layer**: Keras Embedding layer within the RNN model\n",
    "\n",
    "Both approaches will be compared for sentiment classification on Amazon Food Reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278108d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for RNN implementation\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from gensim.models import Word2Vec\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for modeling\n",
    "# Convert labels to binary (0 for negative, 1 for positive)\n",
    "y = final_data['Score'].map({'negative': 0, 'positive': 1}).values\n",
    "X = corpus  # Our preprocessed text data\n",
    "\n",
    "print(f\"Data shape: {len(X)} reviews\")\n",
    "print(f\"Positive reviews: {sum(y)}\")\n",
    "print(f\"Negative reviews: {len(y) - sum(y)}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining set: {len(X_train)} reviews\")\n",
    "print(f\"Test set: {len(X_test)} reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f7048",
   "metadata": {},
   "source": [
    "## Approach 1: Word2Vec + Simple RNN\n",
    "\n",
    "In this approach, we'll:\n",
    "1. Create Word2Vec embeddings from our corpus\n",
    "2. Convert each review to a sequence of word vectors\n",
    "3. Feed these sequences into a Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Word2Vec model\n",
    "# First, tokenize the sentences for Word2Vec training\n",
    "tokenized_corpus = [review.split() for review in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "print(\"Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,  # Embedding dimension\n",
    "    window=5,         # Context window\n",
    "    min_count=2,      # Minimum word frequency\n",
    "    workers=4,        # Number of threads\n",
    "    sg=0,            # CBOW model (sg=1 for Skip-gram)\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"Word2Vec model trained with {len(w2v_model.wv.index_to_key)} words\")\n",
    "print(f\"Embedding dimension: {w2v_model.wv.vector_size}\")\n",
    "\n",
    "# Check some example word vectors\n",
    "sample_words = ['good', 'bad', 'delicious', 'terrible', 'amazing']\n",
    "for word in sample_words:\n",
    "    if word in w2v_model.wv:\n",
    "        print(f\"'{word}' is in vocabulary\")\n",
    "    else:\n",
    "        print(f\"'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert reviews to sequences of word vectors\n",
    "def text_to_word2vec_sequence(text, model, max_length=100):\n",
    "    \"\"\"Convert text to sequence of word vectors\"\"\"\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "        else:\n",
    "            # Use zero vector for unknown words\n",
    "            vectors.append(np.zeros(model.wv.vector_size))\n",
    "    \n",
    "    # Pad or truncate to max_length\n",
    "    if len(vectors) > max_length:\n",
    "        vectors = vectors[:max_length]\n",
    "    else:\n",
    "        # Pad with zero vectors\n",
    "        while len(vectors) < max_length:\n",
    "            vectors.append(np.zeros(model.wv.vector_size))\n",
    "    \n",
    "    return np.array(vectors)\n",
    "\n",
    "# Set maximum sequence length\n",
    "MAX_LEN = 100\n",
    "\n",
    "# Convert training and test data\n",
    "print(\"Converting texts to Word2Vec sequences...\")\n",
    "X_train_w2v = np.array([text_to_word2vec_sequence(text, w2v_model, MAX_LEN) for text in X_train])\n",
    "X_test_w2v = np.array([text_to_word2vec_sequence(text, w2v_model, MAX_LEN) for text in X_test])\n",
    "\n",
    "print(f\"Training data shape: {X_train_w2v.shape}\")\n",
    "print(f\"Test data shape: {X_test_w2v.shape}\")\n",
    "print(f\"Each sequence length: {MAX_LEN}\")\n",
    "print(f\"Each word vector dimension: {w2v_model.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build and train Simple RNN model with Word2Vec embeddings\n",
    "def create_word2vec_rnn_model(input_shape):\n",
    "    \"\"\"Create Simple RNN model for Word2Vec embeddings\"\"\"\n",
    "    model = Sequential([\n",
    "        SimpleRNN(64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "        SimpleRNN(32),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "print(\"Building Word2Vec RNN model...\")\n",
    "w2v_rnn_model = create_word2vec_rnn_model((MAX_LEN, w2v_model.wv.vector_size))\n",
    "\n",
    "# Display model architecture\n",
    "w2v_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec RNN model\n",
    "print(\"Training Word2Vec RNN model...\")\n",
    "history_w2v = w2v_rnn_model.fit(\n",
    "    X_train_w2v, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_w2v.history['loss'], label='Training Loss')\n",
    "plt.plot(history_w2v.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Word2Vec RNN - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_w2v.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_w2v.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Word2Vec RNN - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e773d6",
   "metadata": {},
   "source": [
    "## Approach 2: Keras Embedding Layer + Simple RNN\n",
    "\n",
    "In this approach, we'll:\n",
    "1. Use Keras Tokenizer to convert text to integer sequences\n",
    "2. Use Keras Embedding layer within the RNN model\n",
    "3. Train embeddings end-to-end with the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da42e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize and create integer sequences\n",
    "# Initialize tokenizer\n",
    "vocab_size = 10000  # Maximum number of words to keep\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "\n",
    "# Fit tokenizer on training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to same length\n",
    "MAX_LEN_EMB = 100\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN_EMB, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN_EMB, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Using top {vocab_size} words\")\n",
    "print(f\"Training data shape: {X_train_padded.shape}\")\n",
    "print(f\"Test data shape: {X_test_padded.shape}\")\n",
    "\n",
    "# Show example of tokenization\n",
    "print(f\"\\nExample review: {X_train[0][:100]}...\")\n",
    "print(f\"Tokenized: {X_train_seq[0][:20]}\")\n",
    "print(f\"Padded: {X_train_padded[0][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de890cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build Simple RNN model with Embedding layer\n",
    "def create_embedding_rnn_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create Simple RNN model with Embedding layer\"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SimpleRNN(64, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        SimpleRNN(32),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "embedding_dim = 100\n",
    "print(\"Building Embedding RNN model...\")\n",
    "emb_rnn_model = create_embedding_rnn_model(vocab_size, embedding_dim, MAX_LEN_EMB)\n",
    "\n",
    "# Display model architecture\n",
    "emb_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Embedding RNN model\n",
    "print(\"Training Embedding RNN model...\")\n",
    "history_emb = emb_rnn_model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_emb.history['loss'], label='Training Loss')\n",
    "plt.plot(history_emb.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Embedding RNN - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_emb.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_emb.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Embedding RNN - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b94625",
   "metadata": {},
   "source": [
    "## Model Evaluation and Comparison\n",
    "\n",
    "Let's evaluate both models on the test set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Word2Vec RNN model\n",
    "print(\"=== Word2Vec RNN Model Evaluation ===\")\n",
    "w2v_loss, w2v_accuracy = w2v_rnn_model.evaluate(X_test_w2v, y_test, verbose=0)\n",
    "w2v_predictions = (w2v_rnn_model.predict(X_test_w2v) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"Test Loss: {w2v_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {w2v_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, w2v_predictions, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Evaluate Embedding RNN model\n",
    "print(\"\\n=== Embedding RNN Model Evaluation ===\")\n",
    "emb_loss, emb_accuracy = emb_rnn_model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "emb_predictions = (emb_rnn_model.predict(X_test_padded) > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"Test Loss: {emb_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {emb_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, emb_predictions, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae705642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Word2Vec RNN confusion matrix\n",
    "cm1 = confusion_matrix(y_test, w2v_predictions)\n",
    "sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Negative', 'Positive'],\n",
    "           yticklabels=['Negative', 'Positive'],\n",
    "           ax=axes[0])\n",
    "axes[0].set_title('Word2Vec RNN Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Embedding RNN confusion matrix\n",
    "cm2 = confusion_matrix(y_test, emb_predictions)\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Greens', \n",
    "           xticklabels=['Negative', 'Positive'],\n",
    "           yticklabels=['Negative', 'Positive'],\n",
    "           ax=axes[1])\n",
    "axes[1].set_title('Embedding RNN Confusion Matrix')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b190d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison summary\n",
    "comparison_data = {\n",
    "    'Model': ['Word2Vec RNN', 'Embedding RNN'],\n",
    "    'Test Accuracy': [w2v_accuracy, emb_accuracy],\n",
    "    'Test Loss': [w2v_loss, emb_loss],\n",
    "    'Approach': ['Pre-trained Word2Vec', 'End-to-end Embedding']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"=== Model Comparison Summary ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Test Accuracy'], \n",
    "           color=['skyblue', 'lightgreen'], alpha=0.7)\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i, v in enumerate(comparison_df['Test Accuracy']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Loss comparison\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['Test Loss'], \n",
    "           color=['coral', 'lightpink'], alpha=0.7)\n",
    "axes[1].set_title('Model Loss Comparison')\n",
    "axes[1].set_ylabel('Test Loss')\n",
    "for i, v in enumerate(comparison_df['Test Loss']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on sample reviews\n",
    "def predict_sentiment(text, model_type='embedding'):\n",
    "    \"\"\"Predict sentiment for a given text\"\"\"\n",
    "    if model_type == 'word2vec':\n",
    "        # Preprocess text same as training data\n",
    "        processed_text = re.sub(r'<.*?>', ' ', text)\n",
    "        processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)\n",
    "        processed_text = processed_text.lower()\n",
    "        processed_text = processed_text.split()\n",
    "        processed_text = [lemmatizer.lemmatize(word) for word in processed_text if word not in stop_words]\n",
    "        processed_text = ' '.join(processed_text)\n",
    "        \n",
    "        # Convert to word2vec sequence\n",
    "        sequence = text_to_word2vec_sequence(processed_text, w2v_model, MAX_LEN)\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        prediction = w2v_rnn_model.predict(sequence)[0][0]\n",
    "        \n",
    "    else:  # embedding\n",
    "        # Preprocess text same as training data\n",
    "        processed_text = re.sub(r'<.*?>', ' ', text)\n",
    "        processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)\n",
    "        processed_text = processed_text.lower()\n",
    "        processed_text = processed_text.split()\n",
    "        processed_text = [lemmatizer.lemmatize(word) for word in processed_text if word not in stop_words]\n",
    "        processed_text = ' '.join(processed_text)\n",
    "        \n",
    "        # Convert to sequence\n",
    "        sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "        sequence = pad_sequences(sequence, maxlen=MAX_LEN_EMB, padding='post', truncating='post')\n",
    "        prediction = emb_rnn_model.predict(sequence)[0][0]\n",
    "    \n",
    "    return prediction, 'Positive' if prediction > 0.5 else 'Negative'\n",
    "\n",
    "# Test on sample reviews\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! I love it so much.\",\n",
    "    \"Terrible quality, worst purchase ever. Complete waste of money.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Outstanding flavor and excellent packaging. Highly recommended!\",\n",
    "    \"Poor quality control and bad customer service.\"\n",
    "]\n",
    "\n",
    "print(\"=== Sample Predictions ===\")\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    w2v_score, w2v_sentiment = predict_sentiment(review, 'word2vec')\n",
    "    emb_score, emb_sentiment = predict_sentiment(review, 'embedding')\n",
    "    \n",
    "    print(f\"\\nReview {i+1}: {review}\")\n",
    "    print(f\"Word2Vec RNN:   {w2v_sentiment} (score: {w2v_score:.3f})\")\n",
    "    print(f\"Embedding RNN:  {emb_sentiment} (score: {emb_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9882e2",
   "metadata": {},
   "source": [
    "## Key Insights and Conclusions\n",
    "\n",
    "### Approach Comparison:\n",
    "\n",
    "**1. Word2Vec + RNN:**\n",
    "- **Pros:**\n",
    "  - Uses pre-trained semantic relationships\n",
    "  - Good for capturing word meanings\n",
    "  - Can leverage external word knowledge\n",
    "  \n",
    "- **Cons:**\n",
    "  - More complex preprocessing\n",
    "  - Fixed embedding dimension\n",
    "  - Separate training steps\n",
    "\n",
    "**2. Embedding Layer + RNN:**\n",
    "- **Pros:**\n",
    "  - End-to-end training\n",
    "  - Simpler preprocessing\n",
    "  - Embeddings learned specifically for the task\n",
    "  - More efficient memory usage\n",
    "  \n",
    "- **Cons:**\n",
    "  - Needs sufficient training data\n",
    "  - No external knowledge\n",
    "  - May overfit with small datasets\n",
    "\n",
    "### Performance Notes:\n",
    "- Both models show competitive performance for sentiment analysis\n",
    "- The embedding approach is generally more straightforward for most applications\n",
    "- Word2Vec can be beneficial when you have limited training data or want to leverage external knowledge\n",
    "- RNN models capture sequential patterns in the text effectively\n",
    "\n",
    "### Recommendations:\n",
    "- For production systems, consider using more advanced architectures like LSTM or GRU\n",
    "- Experiment with different embedding dimensions and RNN units\n",
    "- Consider using pre-trained embeddings (GloVe, FastText) for better initialization\n",
    "- Implement proper cross-validation for robust model evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
