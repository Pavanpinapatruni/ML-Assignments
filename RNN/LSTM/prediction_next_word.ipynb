{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4a0ce5",
   "metadata": {},
   "source": [
    "## Project Description: Next Word Prediction Using LSTM\n",
    "#### Project Overview:\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aeacea",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7c7ea",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7854cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits for data: https://www.gutenberg.org/files/1661/1661-0.txt\n",
    "with open('1661-0.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8803bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenizer.word_index.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c35bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad Sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e81aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a621d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into predictors and label\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd854b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=5, \n",
    "                               restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim = total_words, output_dim = 100, input_length=max_sequence_len-1),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    Dropout(0.4),  # Increased dropout\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.4),  # Increased dropout\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc87546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Use a much smaller learning rate\n",
    "optimizer = Adam(learning_rate=0.0001) \n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9319b6",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b6d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = Sequential(\n",
    "    [\n",
    "        Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
    "        GRU(150, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(100),\n",
    "        Dense(total_words, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model_gru.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_gru = model_gru.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58361b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, text_sequence, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text_sequence])[0]\n",
    "\n",
    "    if len(token_list) >= max_sequence_len - 1:\n",
    "        token_list = token_list[-(max_sequence_len - 1):]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I will leave if they\"\n",
    "predicted_word = predict_next_word(model_gru, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Input: '{input_text}' -> \\nPredicted next word: '{predicted_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.save('next_word_gru_model.h5')\n",
    "\n",
    "# Save the model\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
